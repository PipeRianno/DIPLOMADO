{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615fb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Iniciando descarga masiva (hasta 16,000,000 registros)...\n",
      "ğŸ“¥ Descargando lote desde offset 0...\n",
      "ğŸ“¥ Descargando lote desde offset 50,000...\n",
      "ğŸ“¥ Descargando lote desde offset 100,000...\n",
      "ğŸ“¥ Descargando lote desde offset 150,000...\n",
      "ğŸ“¥ Descargando lote desde offset 200,000...\n",
      "ğŸ“¥ Descargando lote desde offset 250,000...\n",
      "ğŸ“¥ Descargando lote desde offset 300,000...\n",
      "ğŸ“¥ Descargando lote desde offset 350,000...\n",
      "ğŸ“¥ Descargando lote desde offset 400,000...\n",
      "ğŸ“¥ Descargando lote desde offset 450,000...\n",
      "ğŸ“¥ Descargando lote desde offset 500,000...\n",
      "ğŸ“¥ Descargando lote desde offset 550,000...\n",
      "ğŸ“¥ Descargando lote desde offset 600,000...\n",
      "ğŸ“¥ Descargando lote desde offset 650,000...\n",
      "ğŸ“¥ Descargando lote desde offset 700,000...\n",
      "ğŸ“¥ Descargando lote desde offset 750,000...\n",
      "ğŸ“¥ Descargando lote desde offset 800,000...\n",
      "ğŸ“¥ Descargando lote desde offset 850,000...\n",
      "ğŸ“¥ Descargando lote desde offset 900,000...\n",
      "ğŸ“¥ Descargando lote desde offset 950,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,000,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,050,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,100,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,150,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,200,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,250,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,300,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,350,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,400,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,450,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,500,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,550,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,600,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,650,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,700,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,750,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,800,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,850,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,900,000...\n",
      "ğŸ“¥ Descargando lote desde offset 1,950,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,000,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,050,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,100,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,150,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,200,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,250,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,300,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,350,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,400,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,450,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,500,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,550,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,600,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,650,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,700,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,750,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,800,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,850,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,900,000...\n",
      "ğŸ“¥ Descargando lote desde offset 2,950,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,000,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,050,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,100,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,150,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,200,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,250,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,300,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,350,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,400,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,450,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,500,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,550,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,600,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,650,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,700,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,750,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,800,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,850,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,900,000...\n",
      "ğŸ“¥ Descargando lote desde offset 3,950,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,000,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,050,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,100,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,150,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,200,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,250,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,300,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,350,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,400,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,450,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,500,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,550,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,600,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,650,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,700,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,750,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,800,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,850,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,900,000...\n",
      "ğŸ“¥ Descargando lote desde offset 4,950,000...\n",
      "ğŸ“¥ Descargando lote desde offset 5,000,000...\n",
      "ğŸ“¥ Descargando lote desde offset 5,050,000...\n",
      "ğŸ“¥ Descargando lote desde offset 5,100,000...\n",
      "ğŸ“¥ Descargando lote desde offset 5,150,000...\n",
      "ğŸ“¥ Descargando lote desde offset 5,200,000...\n",
      "ğŸ“¥ Descargando lote desde offset 5,250,000...\n",
      "ğŸ“¥ Descargando lote desde offset 5,300,000...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.datos.gov.co/resource/rpmr-utcd.json\"\n",
    "DATA_PATH = \"data/secop_completo.csv\"\n",
    "\n",
    "# ParÃ¡metros de descarga\n",
    "LIMIT = 50000  # mÃ¡ximo permitido por Socrata\n",
    "MAX_REGISTROS = 16000000  # mÃ¡ximo estimado en SECOP\n",
    "PAUSA = 1  # segundos entre solicitudes para no saturar el servidor\n",
    "\n",
    "def descargar_todos_los_datos():\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    offset = 0\n",
    "    dfs = []\n",
    "\n",
    "    print(f\"â³ Iniciando descarga masiva (hasta {MAX_REGISTROS:,} registros)...\")\n",
    "\n",
    "    while offset < MAX_REGISTROS:\n",
    "        url = f\"{BASE_URL}?$limit={LIMIT}&$offset={offset}\"\n",
    "        print(f\"ğŸ“¥ Descargando lote desde offset {offset:,}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            data = response.json()\n",
    "            if not data:  # si ya no hay mÃ¡s datos, detener el bucle\n",
    "                print(\"âœ… No hay mÃ¡s datos para descargar.\")\n",
    "                break\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            dfs.append(df)\n",
    "\n",
    "            offset += LIMIT\n",
    "            time.sleep(PAUSA)  # pequeÃ±a pausa para no saturar el servidor\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ Error en el offset {offset}: {e}\")\n",
    "            break\n",
    "\n",
    "    # Unir todos los lotes en un solo DataFrame\n",
    "    if dfs:\n",
    "        df_final = pd.concat(dfs, ignore_index=True)\n",
    "        df_final.to_csv(DATA_PATH, index=False)\n",
    "        print(f\"âœ… Descarga completada: {len(df_final):,} registros guardados en '{DATA_PATH}'\")\n",
    "    else:\n",
    "        print(\"âš  No se descargaron datos.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    descargar_todos_los_datos()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54ae5f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Iniciando descarga masiva... Guardando en data\\secop_completo.parquet\n",
      "ğŸ“¥ Descargando lote desde offset 0...\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… Descarga finalizada. Los datos completos estÃ¡n en \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mdescargar_todos_los_datos\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mdescargar_todos_los_datos\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     59\u001b[39m df = pd.DataFrame(data)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43mguardar_incremental\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodo_append\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m offset += LIMIT\n\u001b[32m     63\u001b[39m time.sleep(PAUSA)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mguardar_incremental\u001b[39m\u001b[34m(df, modo_append)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33;03mGuarda datos en formato Parquet de manera incremental.\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(DATA_PATH) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m modo_append:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     35\u001b[39m     df_existente = pd.read_parquet(DATA_PATH)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanF\\miniconda3\\envs\\Diplomado_env\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanF\\miniconda3\\envs\\Diplomado_env\\Lib\\site-packages\\pandas\\core\\frame.py:3118\u001b[39m, in \u001b[36mDataFrame.to_parquet\u001b[39m\u001b[34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[39m\n\u001b[32m   3037\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3038\u001b[39m \u001b[33;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[32m   3039\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3114\u001b[39m \u001b[33;03m>>> content = f.read()\u001b[39;00m\n\u001b[32m   3115\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[32m-> \u001b[39m\u001b[32m3118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3119\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3126\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanF\\miniconda3\\envs\\Diplomado_env\\Lib\\site-packages\\pandas\\io\\parquet.py:478\u001b[39m, in \u001b[36mto_parquet\u001b[39m\u001b[34m(df, path, engine, compression, index, storage_options, partition_cols, filesystem, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(partition_cols, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    477\u001b[39m     partition_cols = [partition_cols]\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m path_or_buf: FilePath | WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] = io.BytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[32m    482\u001b[39m impl.write(\n\u001b[32m    483\u001b[39m     df,\n\u001b[32m    484\u001b[39m     path_or_buf,\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m     **kwargs,\n\u001b[32m    491\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\juanF\\miniconda3\\envs\\Diplomado_env\\Lib\\site-packages\\pandas\\io\\parquet.py:68\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     65\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m     66\u001b[39m             error_msgs += \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m - \u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mA suitable version of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to import the above resulted in these errors:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     79\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[31mImportError\u001b[39m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "BASE_URL = \"https://www.datos.gov.co/resource/rpmr-utcd.json\"\n",
    "DATA_DIR = \"data\"\n",
    "DATA_PATH = os.path.join(DATA_DIR, \"secop_completo.parquet\")\n",
    "\n",
    "LIMIT = 50000        # mÃ¡ximo permitido por Socrata\n",
    "MAX_REGISTROS = 16000000\n",
    "PAUSA = 1            # segundos entre solicitudes\n",
    "\n",
    "\n",
    "def get_offset_actual():\n",
    "    \"\"\"\n",
    "    Calcula el offset actual segÃºn el tamaÃ±o del archivo Parquet.\n",
    "    Si no existe, comienza desde 0.\n",
    "    \"\"\"\n",
    "    if os.path.exists(DATA_PATH):\n",
    "        df = pd.read_parquet(DATA_PATH)\n",
    "        filas = len(df)\n",
    "        print(f\"âœ… Archivo encontrado con {filas:,} registros. Reanudando desde offset {filas:,}...\")\n",
    "        return filas\n",
    "    return 0\n",
    "\n",
    "\n",
    "def guardar_incremental(df: pd.DataFrame, modo_append=True):\n",
    "    \"\"\"\n",
    "    Guarda datos en formato Parquet de manera incremental.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(DATA_PATH) or not modo_append:\n",
    "        df.to_parquet(DATA_PATH, index=False)\n",
    "    else:\n",
    "        df_existente = pd.read_parquet(DATA_PATH)\n",
    "        df_final = pd.concat([df_existente, df], ignore_index=True)\n",
    "        df_final.to_parquet(DATA_PATH, index=False)\n",
    "\n",
    "\n",
    "def descargar_todos_los_datos():\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    offset = get_offset_actual()\n",
    "    print(f\"â³ Iniciando descarga masiva... Guardando en {DATA_PATH}\")\n",
    "\n",
    "    while offset < MAX_REGISTROS:\n",
    "        url = f\"{BASE_URL}?$limit={LIMIT}&$offset={offset}\"\n",
    "        print(f\"ğŸ“¥ Descargando lote desde offset {offset:,}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if not data:\n",
    "                print(\"âœ… No hay mÃ¡s datos para descargar.\")\n",
    "                break\n",
    "\n",
    "            df = pd.DataFrame(data)\n",
    "            guardar_incremental(df, modo_append=True)\n",
    "\n",
    "            offset += LIMIT\n",
    "            time.sleep(PAUSA)\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"âŒ Error en offset {offset}: {e}\")\n",
    "            break\n",
    "\n",
    "    print(f\"âœ… Descarga finalizada. Los datos completos estÃ¡n en '{DATA_PATH}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    descargar_todos_los_datos()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Diplomado_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
